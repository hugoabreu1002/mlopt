# -*- coding: utf-8 -*-
"""GA_XGBoost_Search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZsytOqW-oP9J3ZiGpQTcJ41wC_rgFLT8
"""

from sklearn.datasets import make_regression
from tqdm import tqdm
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from tqdm import tqdm
import warnings
import xgboost as xgb
from sklearn.metrics import balanced_accuracy_score

"""# Regressor"""

warnings.filterwarnings("ignore")

class GA_XGBRegressor:
    
  def __init__(self,X_train, y_train, X_test, y_test, num_generations=10, size_population=10,
               prob_cruz=0.4, prob_mut=0.5, qt_fits_max=5, max_extimators=100):
    self.data = []
    self.X_train = X_train
    self.y_train = y_train
    self.X_test = X_test
    self.y_test = y_test
    self.num_generations = num_generations
    self.size_population = size_population
    self.prob_cruz = prob_cruz
    self.prob_mut = prob_mut
    self.qt_fits_max = qt_fits_max
    self.max_extimators = max_extimators
    
  def gen_population(self):
    sizepop=self.size_population
      
    population = [['']]*sizepop
    objective = ['reg:squarederror'] #,'reg:tweedie','reg:gamma'
    booster = ['gbtree','gblinear','dart']
    for i in range(0, sizepop):
      population[i] = [np.random.choice(objective), np.random.randint(1, 10), np.random.randint(1, 10),
                       np.random.rand(), np.random.rand(), np.random.randint(20, self.max_extimators),
                       np.random.choice(booster), 10]
      
    return population

  def set_fitness(self, population):
    
    for i in range(0, len(population)):
      xg_reg_volatil = xgb.XGBRegressor(objective=population[i][0], max_depth = population[i][1],
                          min_child_weight = population[i][2], subsample = population[i][3],
                          colsample_bytree = population[i][4], n_estimators =  population[i][5],
                          booster = population[i][6], eval_metric='mae')
     
      qt_fits=0
      mae_fits = np.zeros(self.qt_fits_max)
      while qt_fits < self.qt_fits_max:
        xg_reg_volatil.fit(self.X_train, self.y_train)
        mae_fits[qt_fits] = mean_absolute_error(self.y_test, xg_reg_volatil.predict(self.X_test))
        qt_fits = qt_fits + 1

      population[i][-1] = np.average(mae_fits)
    return population

  def new_gen(self, population):

    def takeLast(elem):
      return elem[:][-1]

    def cruzamento(population, prob_cruz = self.prob_cruz):
      qt_cross = len(population[0])
      pop_ori = population

      for p in range(int(self.size_population*0.1), int(len(pop_ori)/2)):
        if np.random.rand() > prob_cruz:
          population[p][0:int(qt_cross/2)] = pop_ori[2*p][0:int(qt_cross/2)]
        if np.random.rand() > prob_cruz:
          population[p][int(qt_cross/2):qt_cross] = pop_ori[2*p][int(qt_cross/2):qt_cross]

      for p in range(int(self.size_population*0.1), int(len(pop_ori)/2)):
        if np.random.rand() > prob_cruz:
          population[p][0:int(qt_cross/2)] = pop_ori[int(p/2)][0:int(qt_cross/2)]
        if np.random.rand() > prob_cruz:
          population[p][int(qt_cross/2):qt_cross] = pop_ori[int(p/2)][int(qt_cross/2):qt_cross]

      return population

    def mutation(population, prob_mut = self.prob_mut):
      for p in range(int(self.size_population*0.1), len(population)):
        if np.random.rand() > prob_mut:
          population[p][1] = int(population[p][1]*(1+np.random.rand()))
        if np.random.rand() > prob_mut:
          population[p][2] = int(population[p][2]*(1+np.random.rand()))
        if np.random.rand() > prob_mut:
          population[p][3] = np.clip(population[p][3]*(1.5 - np.random.rand()),0,1)
        if np.random.rand() > prob_mut:
          population[p][4] = np.clip(population[p][3]*(1.5 + np.random.rand()),0,1)
        if np.random.rand() > prob_mut:
          population[p][5] = int(population[p][3]*(1+np.random.rand()))

      return population
    
    population = sorted(population, key=takeLast)
    population = cruzamento(population)
    population = mutation(population)
    population = self.set_fitness(population)
    population = sorted(population, key=takeLast)
    
    return population
  
  def search_best(self):
    ng = 0
    population = self.gen_population()
    population = self.set_fitness(population)
    for ng in tqdm(range(0, self.num_generations)):
      population = self.new_gen(population)
      print(population)
    
    def takeLast(elem):
      return elem[:][-1]
    
    best = sorted(population, key=takeLast)[0][:]
    
    return best

X, y = make_regression(n_features=4, n_informative=2,
                       random_state=0, shuffle=False)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

best_params = GA_XGBRegressor(X_train, y_train, X_test, y_test, num_generations=10, size_population=20,
                              prob_cruz=0.2, prob_mut=0.7, qt_fits_max=1, max_extimators=100).search_best()

"""# Classifier"""

class GA_XGBClassifier:

    def __init__(self,X_train, y_train, X_test, y_test, num_generations=10, size_population=10,
               prob_cruz=0.4, prob_mut=0.5, qt_fits_max=5, max_extimators=100):
        
        self.X_train = X_train
        self.y_train = y_train
        self.X_test = X_test
        self.y_test = y_test
        self.num_generations = num_generations
        self.size_population = size_population
        self.prob_cruz = prob_cruz
        self.prob_mut = prob_mut
        self.qt_fits_max = qt_fits_max
        self.max_extimators = max_extimators

    def gen_population(self):
        sizepop=self.size_population

        population = [['']]*sizepop
        objective = ['reg:squarederror','reg:tweedie','reg:gamma'] #,'reg:tweedie','reg:gamma'
        booster = ['gbtree','gblinear','dart']
        for i in range(0, sizepop):
            population[i] = [np.random.choice(objective), np.random.randint(1, 10), np.random.randint(1, 10),
                           np.random.rand(), np.random.rand(), np.random.randint(20, self.max_extimators),
                           np.random.choice(booster), 10]

        return population

    def set_fitness(self, population):
        for i in range(0, len(population)):
            xg_class_volatil = xgb.XGBClassifier(objective=population[i][0], max_depth = population[i][1],
                              min_child_weight = population[i][2], subsample = population[i][3],
                              colsample_bytree = population[i][4], n_estimators =  population[i][5],
                              booster = population[i][6], eval_metric='mae')

            xg_class_volatil.fit(self.X_train, self.y_train)
            mae_fits = balanced_accuracy_score(self.y_test, xg_class_volatil.predict(self.X_test))
            population[i][-1] = mae_fits
            
        return population

    def new_gen(self, population):

        def takeLast(elem):
            return elem[:][-1]

        def cruzamento(population, prob_cruz = self.prob_cruz):
            qt_cross = len(population[0])
            pop_ori = population

            for p in range(int(self.size_population*0.1), int(len(pop_ori)/2)):
                if np.random.rand() > prob_cruz:
                    population[p][0:int(qt_cross/2)] = pop_ori[2*p][0:int(qt_cross/2)]
                if np.random.rand() > prob_cruz:
                    population[p][int(qt_cross/2):qt_cross] = pop_ori[2*p][int(qt_cross/2):qt_cross]

            for p in range(int(self.size_population*0.1), int(len(pop_ori)/2)):
                if np.random.rand() > prob_cruz:
                    population[p][0:int(qt_cross/2)] = pop_ori[int(p/2)][0:int(qt_cross/2)]
                if np.random.rand() > prob_cruz:
                    population[p][int(qt_cross/2):qt_cross] = pop_ori[int(p/2)][int(qt_cross/2):qt_cross]

            return population

        def mutation(population, prob_mut = self.prob_mut):
            for p in range(int(self.size_population*0.1), len(population)):
                if np.random.rand() > prob_mut:
                    population[p][1] = int(population[p][1]*(1+np.random.rand()))
                if np.random.rand() > prob_mut:
                    population[p][2] = int(population[p][2]*(1+np.random.rand()))
                if np.random.rand() > prob_mut:
                    population[p][3] = np.clip(population[p][3]*(1.5 - np.random.rand()),0,1)
                if np.random.rand() > prob_mut:
                    population[p][4] = np.clip(population[p][3]*(1.5 + np.random.rand()),0,1)
                if np.random.rand() > prob_mut:
                    population[p][5] = int(population[p][3]*(1+np.random.rand()))

            return population

        population = sorted(population, key=takeLast, reverse=True)
        population = cruzamento(population)
        population = mutation(population)
        population = self.set_fitness(population)
        population = sorted(population, key=takeLast, reverse=True)

        return population

    def search_best(self):
        population = self.gen_population()
        population = self.set_fitness(population)
        for ng in tqdm(range(0, self.num_generations)):
            population = self.new_gen(population)
            print(population)

        def takeLast(elem):
            return elem[:][-1]

        best = sorted(population, key=takeLast, reverse=True)[0][:]

        return best

from sklearn.datasets import make_gaussian_quantiles
from sklearn.model_selection import train_test_split

# Construct dataset
X1, y1 = make_gaussian_quantiles(cov=2.,
                                 n_samples=200, n_features=2,
                                 n_classes=2, random_state=1)
X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,
                                 n_samples=300, n_features=2,
                                 n_classes=2, random_state=1)
X = np.concatenate((X1, X2))
y = np.concatenate((y1, - y2 + 1))
X_train, X_test, y_train, y_test = train_test_split(X, y)

GA_XGBClassifier(X_train, np.clip(y_train,0,1), X_test, np.clip(y_test,0,1), num_generations=5, size_population=10,
                              prob_cruz=0.5, prob_mut=0.5, max_extimators=100).search_best()

